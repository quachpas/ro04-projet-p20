Liste des paramètres acceptés : address, author, booktitle, chapter, edition, editor, howpublished, institution, journal, key, month, note, number, organization, pages, publisher, school, series, title, type, volume, year, eprint, doi, url, lastchecked
Liste des types d'entrées : article, book, booklet, inbook, incollection, inproceedings, mastersthesis, manual, misc, phdthesius, proceedings, techreport, unpublished

@book{poly:mottelet2003,
  author = "St\'ephane Mottelet",
  title  = "RO04/TI07 - Optimisation non-linéaire",
  howpublished = "Université de Technologie de Compiègne",
  pages = "47-67",
  year = "2003"
}

@book{poly:optimization-purdue,
  title        = {Optimization},
  chapter        = {5},
  howpublished = "Purdue University, West Lafayette, Indiana, USA",
  author       = {S. V. N. Vishwanathan},
  year = "2010",
  pages = "154-"
}
@misc{arxiv:kovalev2019stochastic,
  title={Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates},
  author={Dmitry Kovalev and Konstantin Mishchenko and Peter Richtárik},
  year={2019},
  eprint={1912.01597},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
@article{art:Noack_Funke_2017,
  title={Hybrid genetic deflated Newton method for global optimisation},
  volume={325},
  ISSN={0377-0427},
  DOI={10.1016/j.cam.2017.04.047},
  abstractNote={Optimisation is a basic principle of nature and has a vast variety of applications in research and industry. There is a plurality of different optimisation procedures which exhibit different strengths and weaknesses in computational efficiency and probability of finding the global optimum. Most methods offer a trade-off between these two aspects. This paper proposes a hybrid genetic deflated Newton (HGDN) method to find local and global optima more efficiently than competing methods. The proposed method is a hybrid algorithm which uses a genetic algorithm to explore the parameter domain for regions containing local minima, and a deflated Newton algorithm to efficiently find their exact locations. In each iteration, identified minima are removed using deflation, so that they cannot be found again. The genetic algorithm is adapted as follows: every individual of every generation of offspring searches its adjacent space for optima using Newton’s method; when found, the optimum is removed by deflation, and the individual returns to its starting position. This procedure is repeated until no more optima can be found. The deflation step ensures that the same optimum is not found twice. In the subsequent genetic step, a new generation of offspring is created, using procreation of the fittest. We demonstrate that the proposed method converges to the global optimum, even for small populations. Furthermore, the numerical results show that the HGDN method can improve the number of necessary function and derivative evaluations by orders of magnitude.},
  journal={Journal of Computational and Applied Mathematics},
  author={Noack, Marcus M. and Funke, Simon W.},
  year={2017},
  month={Dec},
  pages={97–112}
}
@article{art:Margossian_2019,
  title={A Review of automatic differentiation and its efficient implementation},
  volume={9},
  ISSN={1942-4787, 1942-4795},
  url={http://arxiv.org/abs/1811.05031},
  DOI={10.1002/WIDM.1305},
  abstractNote={Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of automatic differentiation however requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open problems include the extension of current packages to provide more specialized routines, and efficient methods to perform higher-order differentiation.},
  note={arXiv: 1811.05031},
  number={4},
  journal={WIREs Data Mining and Knowledge Discovery},
  author={Margossian, Charles C.},
  year={2019},
  month={Jul}
}
@article{in:Greenstadt_1970,
  title={Variations on Variable-Metric Methods},
  DOI={10.2307/2004872},
  abstractNote={In unconstrained minimization of a functions, the method of Davidon-Fletcher- Powell (a “variable-metric” method) enables the inverse of the Hessian H of f to be ap- proximated stepwise, using only values of the gradient of f. It is shown here that, by solv- ing a certain variational problem, formulas for the successive corrections to H can be de- rived which closely resemble Davidon’s. A symmetric correction matrix is sought which minimizes a weighted Euclidean norm, and also satisfies the “DFP condition.” Numerical tests are described, comparing the performance (on four “standard” test functions) of two variationally-derived formulas with Davidon’s. A proof by Y. Bard, modelled on Fletcher and Powell’s, showing that the new formulas give the exact H after N steps, is included in an appendix. 1. The DFP Method. The class of gradient methods for finding the uncon- strained minimum of a function f(x)* in which the direction Sk of the next iterative step from Xk to Xk+l is computed from a formula such as:},
  author={Greenstadt, John},
  year={1970}
}
@article{art:Davidon_1991,
  title={Variable Metric Method for Minimization},
  volume={1},
  ISSN={1052-6234},
  DOI={10.1137/0801001},
  abstractNote={This is a method for determining numerically local minima of differentiable functions of several variables. In the process of locating each minimum, a matrix which characterizes the behavior of the function about the minimum is determined. For a region in which the function depends quadratically on the variables, no more than N iterations are required, where N is the number of variables. By suitable choice of starting values, and without modification of the procedure, linear constraints can be imposed upon the variables.},
  number={1},
  journal={SIAM Journal on Optimization},
  author={Davidon, William C.},
  year={1991},
  month={Feb},
  pages={1–17}
}
@article{art:Broyden_1967,
  title={Quasi-Newton methods and their application to function minimisation},
  volume={21},
  ISSN={0025-5718, 1088-6842},
  DOI={10.1090/S0025-5718-1967-0224273-2},
  abstractNote={Advancing research. Creating connections.},
  number={99},
  journal={Mathematics of Computation},
  author={Broyden, C. G.},
  year={1967},
  pages={368–381}
}
@article{art:Broyden_1970,
  title={The Convergence of a Class of Double-rank Minimization Algorithms 1. General Considerations},
  DOI={10.1093/imamat/6.1.76},
  abstractNote={This paper presents a more detailed analysis of a class of minimization algorithms, which includes as a special case the DFP (Davidon-Fletcher-Powell) method, than has previously appeared. Only quadratic functions are considered but particular attention is paid to the magnitude of successive errors and their dependence upon the initial matrix. On the basis of this a possible explanation of some of the observed characteristics of the class is tentatively suggested. PROBABLY the best-known algorithm for determining the unconstrained minimum of a function of many variables, where explicit expressions are available for the first partial derivatives, is that of Davidon (1959) as modified by Fletcher & Powell (1963). This algorithm has many virtues. It is simple and does not require at any stage the solution of linear equations. It minimizes a quadratic function exactly in a finite number of steps and this property makes convergence of this algorithm rapid, when applied to more general functions, in the neighbourhood of the solution. It is, at least in theory, stable since the iteration matrix H,, which transforms the jth gradient into the /th step direction, may be shown to be positive definite. In practice the algorithm has been generally successful, but it has exhibited some puzzling behaviour. Broyden (1967) noted that H, does not always remain positive definite, and attributed this to rounding errors. Pearson (1968) found that for some problems the solution was obtained more efficiently if H, was reset to a positive definite matrix, often the unit matrix, at intervals during the computation. Bard (1968) noted that H, could become singular, attributed this to rounding error and suggested the use of suitably chosen scaling factors as a remedy. In this paper we analyse the more general algorithm given by Broyden (1967), of which the DFP algorithm is a special case, and determine how for quadratic functions the choice of an arbitrary parameter affects convergence. We investigate how the successive errors depend, again for quadratic functions, upon the initial choice of iteration matrix paying particular attention to the cases where this is either the unit matrix or a good approximation to the inverse Hessian. We finally give a tentative explanation of some of the observed experimental behaviour in the case where the function to be minimized is not quadratic.},
  author={Broyden, C. G.},
  year={1970}
}
@article{art:Fletcher1970,
  title={A new approach to variable metric algorithms},
  volume={13},
  ISSN={0010-4620},
  DOI={10.1093/comjnl/13.3.317},
  abstractNote={Abstract.  An approach to variable metric algorithms has been investigated in which the linear search sub-problem no longer becomes necessary. The property of q},
  number={3},
  journal={The Computer Journal},
  author={Fletcher, R.},
  year={1970},
  month={Jan},
  pages={317–322}
}
@article{art:Fliege_Drummond_Svaiter_2009,
  title={Newton’s Method for Multiobjective Optimization},
  DOI={10.1137/08071692X},
  abstractNote={We propose an extension of Newton’s method for unconstrained multiobjective optimization (multicriteria optimization). This method does not use a priori chosen weighting factors or any other form of a priori ranking or ordering information for the different objective functions. Newton’s direction at each iterate is obtained by minimizing the max-ordering scalarization of the variations on the quadratic approximations of the objective functions. The objective functions are assumed to be twice continuously differentiable and locally strongly convex. Under these hypotheses, the method, as in the classical case, is locally superlinear convergent to optimal points. Again as in the scalar case, if the second derivatives are Lipschitz continuous, the rate of convergence is quadratic. Our convergence analysis uses a Kantorovich-like technique. As a byproduct, existence of optima is obtained under semilocal assumptions.},
  journal={SIAM J. Optim.},
  author={Fliege, Jörg and Drummond, L. Mauricio Graña and Svaiter, Benar Fux},
  year={2009}
}
@article{art:McSweeney_2017, 
title={Modified Cholesky Decomposition and Applications}, 
abstractNote={The modified Cholesky decomposition is one of the standard tools in various areas of mathematics for dealing with symmetric indefinite matrices that are required to be positive definite. We survey the literature and determine which of the existing modified Cholesky algorithms is most suitable for inclusion in the Numerical Algorithms Group (NAG) software library, focussing in particular on the algorithms of Gill, Murray and Wright, Schnabel and Eskow, Cheng and Higham, and More and Sorensen. In order to make this determination we consider how best to take advantage of modern computer architectures and existing numerical software. We create an efficient implementation of the chosen algorithm and perform extensive numerical testing to ensure that it works as intended. We then discuss various applications of the modified Cholesky decomposition and show how the new implementation can be used for some of these. In particular, significant attention is devoted to describing how the modified Cholesky decomposition can be used to compute an upper bound on the distance to the nearest correlation matrix.}, 
author={McSweeney, Thomas}, 
year={2017} 
}
@book{book:Floudas_Pardalos_2008,
  title={Encyclopedia of Optimization},
  ISBN={9780387747583},
  abstractNote={The goal of the Encyclopedia of Optimization is to introduce the reader to a complete set of topics that show the spectrum of research, the richness of ideas, and the breadth of applications that has come from this field. The second edition builds on the success of the former edition with more than 150 completely new entries, designed to ensure that the reference addresses recent areas where optimization theories and techniques have advanced. Particularly heavy attention resulted in health science and transportation, with entries such as “Algorithms for Genomics”, “Optimization and Radiotherapy Treatment Design”, and “Crew Scheduling”.},
  note={Google-Books-ID: 1a6lSRbQ4YsC},
  publisher={Springer Science \& Business Media},
  author={Floudas, Christodoulos A. and Pardalos, Panos M.},
  year={2008},
  month={Sep}
}
@book{book:SummaryofOptimizers,
  title = "Advanced Design System 2008 - Tuning, Optimization, and Statistical Design",
  author = "Agilent Technologies",
  month = "January",
  year = "2008",
  url={http://literature.cdn.keysight.com/litweb/pdf/ads2008/optstat/ads2008/Summary_of_Optimizers.html#SummaryofOptimizers-SimulatedAnnealingOptimizer}
}
@book{book:newtonfluxions,
author    = "Isaac Newton (1642-1727",
title     = "La méthode des fluxions, et les suites infinies, par M. le chevalier Newton",
publisher = "Georges-Louis Leclerc Buffon (1707-1788 ; comte de). Traducteur",
year      = "1740",
url = "https://gallica.bnf.fr/ark:/12148/bpt6k62411f",
note = "[En ligne; Source : gallica.bnf.fr / BnF]"
}

@book{book:raphsonanalysis,
  author    = {Joseph Raphson (ca 1648- ca 1715)},
  title     = {Analysis aequationum universalis seu ad aequationes algebraicas resolvendas methodus generalis, \& expedita, ex nova infinitarum serierum methodo, deducta ac demonstrata},
  year      = {1690}
}
@book{book:Fletcher_1987,
  author         = {Fletcher, R. (Roger)},
  title          = {Practical methods of optimization},
  publisher      = {Chichester, New York, Wiley},
  year           = {1987},
  abstractNote   = {xiv, 436 p. : 24 cm; "A Wiley-Interscience publication."; Bibliography: p. [417]-429; Includes index},
  url            = {http://archive.org/details/practicalmethods0000flet},
}
@misc{web:sci-BOT,
  author       = {Lydia E. van Dijk, Christoph L. Spiel},
  title        = {sci-BOT - the Scilab Bag Of Tricks - Chapter 6. Performance},
  url            = {http://kiwi.emse.fr/SCILAB/sci-bot/book1.htm},
  year         = {2000},
  note           = {[En ligne; Page disponible le 24-juin-2020]},
}
@misc{doc:scilab-backslash,
  author       = {Scilab Enterprises},
  title        = {backslash},
  url            = {https://help.scilab.org/docs/6.1.0/en_US/backslash.html},
  year         = {2020},
  note           = {[En ligne; Page disponible le 25-février-2020]},
}
@misc{wiki:frmethodenewton,
author = "Wikipédia",
title = "Méthode de Newton --- Wikipédia{,} l'encyclopédie libre",
year = "2020",
url = "http://fr.wikipedia.org/w/index.php?title=M%C3%A9thode_de_Newton&oldid=171999014",
note = "[En ligne; Page disponible le 14-juin-2020]"
}

@misc{wiki:fonctionholomorphe,
author = "Wikipédia",
title = "Fonction holomorphe --- Wikipédia{,} l'encyclopédie libre",
year = "2020",
url = "http://fr.wikipedia.org/w/index.php?title=Fonction_holomorphe&oldid=168539915",
note = "[En ligne; Page disponible le 18-mars-2020]"
}


@misc{wiki:ennewtonoptimization,
author = "{Wikipedia contributors}",
title = "Newton's method in optimization --- {Wikipedia}{,} The Free Encyclopedia",
year = "2020",
url = "https://en.wikipedia.org/w/index.php?title=Newton%27s_method_in_optimization&oldid=956681627",
note = "[Online; accessed 4-June-2020]"
}
@misc{wiki:enquasinewton,
author = "{Wikipedia contributors}",
title = "Quasi-Newton method --- {Wikipedia}{,} The Free Encyclopedia",
year = "2020",
url = "https://en.wikipedia.org/w/index.php?title=Quasi-Newton_method&oldid=959247803",
note = "[Online; accessed 4-June-2020]"
}

@misc{wiki:frbfgs,
author = "Wikipédia",
title = "Méthode de Broyden-Fletcher-Goldfarb-Shanno --- Wikipédia{,} l'encyclopédie libre",
year = "2020",
url = "http://fr.wikipedia.org/w/index.php?title=M%C3%A9thode_de_Broyden-Fletcher-Goldfarb-Shanno&oldid=171395807",
note = "[En ligne; Page disponible le 28-mai-2020]"
}

@misc{wiki:en-bfgs,
author = "{Wikipedia contributors}",
title = "Broyden–Fletcher–Goldfarb–Shanno algorithm --- {Wikipedia}{,} The Free Encyclopedia",
year = "2020",
url = "https://en.wikipedia.org/w/index.php?title=Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm&oldid=950677423",
note = "[Online; accessed 4-June-2020]"
}
@misc{wiki:raphson,
author = "{Wikipedia contributors}",
title = "Joseph Raphson --- {Wikipedia}{,} The Free Encyclopedia",
year = "2020",
url = "https://en.wikipedia.org/w/index.php?title=Joseph_Raphson&oldid=946169368",
note = "[Online; accessed 5-June-2020]"
}
@misc{ wiki:vitesse_convergence,
author = "Wikipédia",
title = "Vitesse de convergence des suites --- Wikipédia{,} l'encyclopédie libre",
year = "2020",
url = "http://fr.wikipedia.org/w/index.php?title=Vitesse_de_convergence_des_suites&oldid=169466597",
note = "[En ligne; Page disponible le 12-avril-2020]"
}
@misc{ wiki:xxx,
author = "{Wikipedia contributors}",
title = "Limited-memory BFGS --- {Wikipedia}{,} The Free Encyclopedia",
year = "2020",
url = "https://en.wikipedia.org/w/index.php?title=Limited-memory_BFGS&oldid=962019390",
note = "[Online; accessed 23-June-2020]"
}