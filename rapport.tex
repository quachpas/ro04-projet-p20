% Dépendences : 
% pip install pygments
% Python3

% Installation de pip : (Windows)
% wget https://bootstrap.pypa.io/get-pip.py
% python3 get-pip.py
% Rajout dans le PATH, cf. message

% Compilation : 
% 1 - pdflatex --shell-escape rapport.tex
% 2 - bibtex --shell-escape rapport.aux
% 3 - pdflatex --shell-escape rapport.tex


% Citations de sources bibliographiques
% http://merkel.texture.rocks/Latex/natbib.php?lang=fr

% Documentation classe 
% https://www.elsevier.com/__data/assets/pdf_file/0009/56844/elsdoc2.pdf
\documentclass[1p]{elsarticle}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[french]{babel}
\usepackage{fancyhdr}
\usepackage{titlesec} % titlespacing %
\usepackage{listings} % lstnewenvironment %
\usepackage{textcomp}
\usepackage{regexpatch}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{caption}
\usepackage{titletoc}
\usepackage[table]{xcolor}
\usepackage{minted}
\usepackage[french]{algorithm2e}% http://tug.ctan.org/macros/latex/contrib/algorithm2e/doc/algorithm2e.pdf
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\hypersetup{
  colorlinks = false,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black
}
\newtheorem{thm}{Théorème}
\newtheorem{lem}[thm]{Lemme}
\newtheorem{definition}{Définition}[section]
\newdefinition{rmk}{Remarque}
\newproof{pf}{Preuve}

\newcounter{linecounter}
\newcommand{\linenumbering}{\ifthenelse{\value{linecounter}<10}{(0\arabic{linecounter})}{(\arabic{linecounter})}}
\renewcommand{\line}[1]{\refstepcounter{linecounter}\label{#1}\linenumbering}
\newcommand{\resetline}[1]{\setcounter{linecounter}{0}#1}
\renewcommand{\thelinecounter}{\ifnum \value{linecounter} > 9\else 0\fi \arabic{linecounter}}

% Enlever le footer spécifique à Elsevier

\keywordtitle{Mots-clés\,}
\abstracttitle{Résumé}

\makeatletter
\regexpatchcmd*{\@makecaption}{:}{\cA:}{}{}
\regexpatchcmd*{\keyword}{:}{\cA:}{}{}
\def\ps@pprintTitle{%
 \let\@oddhead\@empty
 \let\@evenhead\@empty
 \def\@oddfoot{}%
 \let\@evenfoot\@oddfoot}
% fix english
\xpatchcmd{\printFirstPageNotes}
  {Email addresses}
  {Adresses email}{}{}
\xpatchcmd{\printFirstPageNotes}
  {Email address}
  {Adresse email}{}{}
\regexpatchcmd*{\printFirstPageNotes}{:}{\cA:}{}{}
\makeatother

\begin{document}
\begin{frontmatter}
    \title{Méthode de Newton et de Quasi-Newton BFGS (Algorithme de BRoyden, Fletcher, Goldfarb et Shanno)}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{logo_UTC.jpg}
    \end{center}
    \author{Mathilde Rineau}
    \author{Mathilde Le Moel}
    \author{Pascal Quach}
    \author{Félix Poullet-Pagès}

    \begin{abstract}
        La méthode de Newton est une méthode itérative de recherche d'un zéro d'une fonction $g$, qui se repose sur la méthode du point fixe. Elle requiert cependant le calcul coûteux de la dérivée ou matrice jacobienne de $g$ dans le cas d'une fonction à plusieurs variables, et également son inversion. On parle de méthodes de quasi-Newton lorsque la matrice jacobienne - et généralement son inverse - sont remplacées par une approximation. Appliquée à un problème d'optimisation, où l'on cherche l'optimum d'une fonction $f$, la méthode de Newton force le calcul de la matrice hessienne de $f$, car on cherche les zéros du gradient de $f$. La méthode de Broyden-Fletcher-Goldfarb-Shanno (BFGS) est une méthode quasi-Newton qui se repose sur l'approximation de la matrice hessienne par analyse des gradients successifs. La matrice hessienne n'est pas calculée à chaque itération de la méthode, mais mise à jour itérativement en prenant une estimation de la matrice hessienne initiale.
    \end{abstract}
\end{frontmatter}
\clearpage
\tableofcontents

\clearpage
\section{Notations}
\begin{enumerate}
    \item $\nabla f(x)$ désigne le gradient de la fonction $f$ au point $x$, c'est-à-dire la matrice jacobienne de $f$ évaluée en $x$.
    \item $\nabla ^2 f(x)$ désigne le hessien de la fonction $f$ au point $x$, c'est-à-dire la matrice hessienne de $f$ évaluée en $x$.
\end{enumerate}
\section{Méthode de Newton}
\subsection{Origines}
La méthode de Newton apparaît pour la première fois dans l'ouvrage \emph{De analysi per aequationes numero terminorum infinitas} (1669), puis une seconde fois dans \textit{La méthode des fluxions\footnote{Les fluxions représentent des dérivées}, et les suites infinies}\footnote{Voir \cite{book:newton_fluxions}}, tous deux écrits par Isaac Newton. Elle est à l'époque appliquée au cas particulier des recherches des racines d'un polynôme. Ses travaux ne seront malheureusement publiés que de façon posthume à partir de 1736. Cette méthode est simplifiée par Joseph Raphson dans \textit{Analysis aequationum universalis} (1690), et c'est généralement la version de Raphson qui est utilisée aujourd'hui. Pour ces raisons, on a tendance à parler de la méthode de Newton-Raphson.
\begin{figure}[htbp]
    \centering
    \includegraphics[width = 0.5\textwidth]{La_méthode_des_fluxions_et_[...]Newton_Isaac_bpt6k62411f_40.jpg}
    \caption{Extrait de l'ouvrage \textit{La méthode
    des fluxions, et les suites infinies} expliquant la méthode de Newton}
    \label{fig:fluxion_newton_method}
\end{figure}

\subsection{Principe}
La méthode de Newton-Raphson est une méthode de recherche des racines d'une fonction à valeurs réelles.
\begin{definition}{Principe}
    Étant donné une fonction $g: \mathbb{R}^n\longrightarrow \mathbb{R}$, $g\in C^2$ et une valeur initiale $x_0\in \mathbb{R}^n$, alors la suite $(x_k)_{k\in \mathbb{N}}$ définie par 
    \begin{equation}
        x_{k+1} = x_k - \nabla g(x_k)^{-1}\cdot g(x_k)
    \end{equation}
    converge vers la racine $x_*$ de $g$ la plus proche de $x_0$, à condition que $x_0$ soit suffisamment proche de $x_*$.     
\end{definition}
Si on prend l'exemple 

\begin{figure}[htbp]
    \centering
    \input{iterations-newton-raphson.tex}
    \caption{Itérations de l'algorithme de Newton-Raphson pour une $g(x)=x^2-3$}
    \label{fig:nr-iterations}
\end{figure}

\subsection{Résultats connus}
\subsection{Qualités et faiblesses}
\section{Méthode de BFGS}
\subsection{Origines}
\subsection{Principe}
\subsection{Résultats connus}
\subsection{Qualités et faiblesses}
\section{Implémentation sur SCILAB}
\subsection{Recherche linéaire : condition de Wolfe}
\subsubsection{Algorithme}
\subsubsection{Programme}
\subsubsection{Conditions d'arrêts et précision}
\subsubsection{Difficultées techniques}
\subsection{Méthode de Newton}
\subsubsection{Algorithme}
\begin{algorithm}[ht]
    \centering{
        \fbox{
            \begin{minipage}[t]{150mm}
                \footnotesize
                \renewcommand{\baselinestretch}{2.5}
                \resetline
                \begin{tabbing}
                    aaaA\=aaaA\=aaA\=aaA\=aaA\=aaA\kill

                    {\bf  Initialisation :}\\
                    \line{NR-01} \> $g$ := la fonction dont on cherche les racines\\
                    \line{NR-02} \> $\nabla g$ := l'expression analytique du gradient de $g$\\
                    \line{NR-03} \> $k = 0$ := le nombre d'itérations actuels\\
                    \line{NR-04} \> $x_0 \in \mathbb{R}^n$ := la valeur initiale de recherche\\
                    \line{NR-05} \> $\varepsilon \in \mathbb{R}^+_*$ := la marge d'erreur acceptable, le critère d'arrêt\\
                    \line{NR-06} \> $N\in \mathbb{N}$ := le nombre d'itérations maximales\\
                    {\bf Itérations :}\\
                    \line{NR-07} \> {\bf Tant que} $g(x_k) > \varepsilon$ {\bf ET} $k < N$ {\bf faire :}\\
                    \line{NR-08} \>\> $x_{k+1} = x_k - \nabla g^{-1}(x_k)\cdot g(x_k)$\\
                    \line{NR-09} \>\> $N = N+1$\\
                    \line{NR-10} \>\> $k = k+1$\\
                    %------------------
                \end{tabbing}
                \normalsize
            \end{minipage}
        }
        \medbreak
        \caption{Algorithme de la méthode de Newton-Raphson}
        \label{algo:newton_raphson}
    }
\end{algorithm}
\subsubsection{Programme}
\subsubsection{Conditions d'arrêts et précision}
\subsubsection{Difficultées techniques}
\subsection{Méthode de BFGS}
\subsubsection{Algorithme}
\subsubsection{Programme}
\subsubsection{Conditions d'arrêts et précision}
\subsubsection{Difficultées techniques}
\subsection{Affichage des résultats}
\subsubsection{Indicateurs possibles}
\subsection{Exemples d'optimisation}
\subsubsection{Méthode de Newton}
% \paragraph{Exemple convergent}
% \paragraph{Exemple divergent}
% \paragraph{Exemple de convergence difficile}
\subsubsection{Méthode de BFGS}
% \paragraph{Exemple convergent}
% \paragraph{Exemple divergent}
% \paragraph{Exemple de convergence difficile}
\subsection{Critiques et alternatives}
\subsubsection{Critiques}
% \paragraph{Prise en main du langage}
% \paragraph{Temps de calcul}
\subsubsection{Alternatives}
\section{Applications}
\subsection{Problème 1 : ...}
% \paragraph{Indicateur 1 : courbe de convergence (exemple)}



% Bibliographie
% http://merkel.texture.rocks/Latex/natbib.php?lang=fr
\bibliographystyle{elsarticle-num-names}
\bibliography{bibliographie}

\phantomsection
\addcontentsline{toc}{section}{Références}

\end{document}
