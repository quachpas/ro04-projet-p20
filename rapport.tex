% Dépendences : 
% pip install pygments
% Python3

% Installation de pip : (Windows)
% wget https://bootstrap.pypa.io/get-pip.py
% python3 get-pip.py
% Rajout dans le PATH, cf. message

% Compilation : 
% 1 - pdflatex --shell-escape rapport.tex
% 2 - bibtex --shell-escape rapport.aux
% 3 - pdflatex --shell-escape rapport.tex


% Citations de sources bibliographiques
% http://merkel.texture.rocks/Latex/natbib.php?lang=fr

% Documentation classe 
% https://www.elsevier.com/__data/assets/pdf_file/0009/56844/elsdoc2.pdf
\documentclass[3p, twocolumn]{elsarticle}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[french]{babel}
\usepackage{fancyhdr}
\usepackage{physics}
\usepackage{titlesec} % titlespacing %
\usepackage{listings} % lstnewenvironment %
\usepackage{textcomp}
\usepackage{regexpatch}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{caption}
\usepackage{titletoc}
\usepackage[table]{xcolor}
\usepackage{minted}
\usepackage[french]{algorithm2e}% http://tug.ctan.org/macros/latex/contrib/algorithm2e/doc/algorithm2e.pdf
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage[toc,page]{appendix}
\renewcommand{\appendixtocname}{Annexes}
\renewcommand{\appendixpagename}{Annexes}
\DeclareMathOperator{\Hessian}{H}
\DeclareMathOperator{\Jacobian}{J}
\hypersetup{
  colorlinks = false,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black
}
\newtheorem{thm}{Théorème}
\newtheorem{lem}[thm]{Lemme}
\newtheorem{definition}{Définition}[section]
\newdefinition{rmk}{Remarque}[section]
\newdefinition{exemple}{Exemple}[section]
\newproof{pf}{Preuve}[section]

\newcounter{linecounter}
\newcommand{\linenumbering}{\ifthenelse{\value{linecounter}<10}{(0\arabic{linecounter})}{(\arabic{linecounter})}}
\renewcommand{\line}[1]{\refstepcounter{linecounter}\label{#1}\linenumbering}
\newcommand{\resetline}[1]{\setcounter{linecounter}{0}#1}
\renewcommand{\thelinecounter}{\ifnum \value{linecounter} > 9\else 0\fi \arabic{linecounter}}

% Enlever le footer spécifique à Elsevier

\keywordtitle{Mots-clés\,}
\abstracttitle{Résumé}

\makeatletter
\regexpatchcmd*{\@makecaption}{:}{\cA:}{}{}
\regexpatchcmd*{\keyword}{:}{\cA:}{}{}
\def\ps@pprintTitle{%
 \let\@oddhead\@empty
 \let\@evenhead\@empty
 \def\@oddfoot{}%
 \let\@evenfoot\@oddfoot}
% fix english
\xpatchcmd{\printFirstPageNotes}
  {Email addresses}
  {Adresses email}{}{}
\xpatchcmd{\printFirstPageNotes}
  {Email address}
  {Adresse email}{}{}
\regexpatchcmd*{\printFirstPageNotes}{:}{\cA:}{}{}
\makeatother

\begin{document}
\begin{frontmatter}
    \title{Méthode de Newton et de Quasi-Newton BFGS (Algorithme de BRoyden, Fletcher, Goldfarb et Shanno)}
    \author{Mathilde Rineau}
    \author{Mathilde Le Moel}
    \author{Pascal Quach}
    \author{Félix Poullet-Pagès}

    \begin{abstract}
        La méthode de Newton est une méthode itérative de recherche d'un zéro d'une fonction $g$, qui se repose sur la méthode du point fixe. Elle requiert cependant le calcul coûteux de la dérivée ou matrice jacobienne de $g$ dans le cas d'une fonction à plusieurs variables, et également son inversion. On parle de méthodes de quasi-Newton lorsque la matrice jacobienne - et généralement son inverse - sont remplacées par une approximation. Appliquée à un problème d'optimisation, où l'on cherche l'optimum d'une fonction $f$, la méthode de Newton force le calcul de la matrice hessienne de $f$, car on cherche les zéros du gradient de $f$. La méthode de Broyden-Fletcher-Goldfarb-Shanno (BFGS) est une méthode quasi-Newton qui se repose sur l'approximation de la matrice hessienne par analyse des gradients successifs. La matrice hessienne n'est pas calculée à chaque itération de la méthode, mais mise à jour itérativement en prenant une estimation de la matrice hessienne initiale.
    \end{abstract}
\end{frontmatter}

\newpage
\tableofcontents

\newpage

\clearpage
\section{Notations}
\begin{enumerate}
    \item $\Jacobian_{f}(x)$ désigne la matrice jacobienne de la fonction $f$ évaluée en $x$.
    \item $\Hessian_{f}(x)$ désigne la matrice hessienne de la fonction $f$ évaluée en $x$.
\end{enumerate}
\section{Méthode de Newton}
\subsection{Origines}
La méthode de Newton apparaît pour la première fois au XVIIème siècle dans l'ouvrage \emph{De analysi per aequationes numero terminorum infinitas} (1669), puis une seconde fois dans \textit{La méthode des fluxions\footnote{Les fluxions représentent des dérivées}, et les suites infinies}\footnote{Voir \cite{book:newton_fluxions}}, tous deux écrits par Isaac Newton. Elle est à l'époque appliquée au cas particulier des recherches des racines d'un polynôme. Ses travaux ne seront malheureusement publiés que de façon posthume à partir de 1736. Cette méthode est simplifiée par Joseph Raphson dans \textit{Analysis aequationum universalis} (1690), et c'est généralement la version de Raphson qui est utilisée aujourd'hui. Pour ces raisons, on a tendance à parler de la méthode de Newton-Raphson. Les travaux de Thomas Simpson élargissent cette méthode itérative à la recherche de racines d'un plus grande nombre d'équations, notamment les équations non linéaires.
\begin{figure}[htbp]
    \centering
    \includegraphics[width = 0.3\textwidth]{La_méthode_des_fluxions_et_[...]Newton_Isaac_bpt6k62411f_40.jpg}
    \caption{Extrait de l'ouvrage \textit{La méthode
            des fluxions, et les suites infinies} expliquant la méthode de Newton}
    \label{fig:fluxion_newton_method}
\end{figure}

\subsection{Principe}
La méthode de Newton-Raphson est une méthode de recherche des racines de fonctions à valeurs réelles.
\begin{definition}
    Étant donné une fonction $G: \Omega\subset\mathbb{R}^n\longrightarrow \mathbb{R}^n$, $G\in C^1$ et une valeur initiale $x_0\in \Omega$, alors la suite $(x_k)_{k\in \mathbb{N}}$ définie par
    \begin{equation}
        x_{k+1} = x_k - \Jacobian^{-1}_G(x)\cdot G(x_k)
        \label{eq:iteration-newton-rn}
    \end{equation}
    converge vers la racine $x_*$ de $G$, à condition que $x_0$ soit suffisamment proche de $x_*$.
\end{definition}
\begin{rmk}
    $\Jacobian_G$ doit être inversible.
\end{rmk}
\begin{rmk}
    Il est important que la fonction $G$ admette un développement de Taylor d'ordre 1 dans un voisinage des racines $x_*$. En effet, l'existence de ce développement justifie la convergence de la méthode itérative.
    On considère le développement de Taylor d'ordre 1 au voisinage de $x_{k+1}$.
    \begin{equation*}
        G(x_{k+1})\simeq G(x_k)+\Jacobian_G(x_k)(x_{k+1}-x_k)
    \end{equation*}
    Pour s'approcher d'une racine de $G$, on considère que $G(x_{k+1})=0$, et on en déduit l'expression de l'équation \ref{eq:iteration-newton-rn}.
\end{rmk}

\subsection{Interprétation géométrique}
Pour comprendre cette méthode itérative, il est plus simple de raisonner en deux dimensions. On considère la fonction $g :\Omega\subset\mathbb R^2 \rightarrow \mathbb R$.

La formule itérative équivalente est la suivante :
\begin{equation}
    x_{k+1}=x_k-\frac{g(x_k)}{g'(x_k)}
    \label{eq:iteration-newton-r2}
\end{equation}
En passant tous les membres à gauche dans l'équation \ref{eq:iteration-newton-r2}, on se retrouve avec $g'(x_k)(x_{k+1}-x_k)+g(x_k)=0$. On reconnaît ici l'équation de la tangente à $g$ au point $x=x_k$ évaluée au point $(x_{k+1},0)$.
Le point $(x_{k+1},0)$ correspond à l'intersection entre la tangente à $g$ au point $x_k$ et la droite des abscisses.

La figure \ref{fig:nr-iterations-1} fournit un exemple visuel des itérations de la méthode de Newton-Raphson en deux dimensions. La fonction $g$ est définie par $g(x)=\frac{1}{12}x^3+\frac18x^2-3x$, et on choisit comme valeur de recherche initiale $x_0=-3.5$. On dessine les tangentes successives, et leurs intersections, jusqu'à s'approcher raisonnablement d'une racine de $g$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width = 0.3\textwidth]{iteration-newton-1.png}
    \caption{Itérations de l'algorithme de Newton-Raphson pour la fonction $g$ définie par $g(x)=\frac{1}{12}x^3+\frac18x^2-3x$ telle que $x_0=-3.5$}
    \label{fig:nr-iterations-1}
\end{figure}

\begin{rmk}
    Le choix de la valeur de recherche initiale est important. Si l'on choisit $x_0=-4$, la pente de la tangente de $g$ en $x_0$ est nulle, et les itérations suivantes ne donnent pas de valeurs finies car la dérivée est nulle : la méthode itérative s'arrête. Ce choix a tout autant d'importance en dimensions supérieures à 2.
\end{rmk}

\subsection{Cas de non-convergence}
\subsubsection{Domaine de définition}
Le calcul de $x_{k+1}$ peut mener à une valeur en dehors du domaine de définition $\Omega$ de $G$.
\subsubsection{Différentiabilité}
Il n'existe pas de développement de Taylor au voisinage de la racine $x_*$. C'est-à-dire que $G$ n'est pas différentiable en $x_*$          
\begin{exemple}
    On considère la fonction $G:\mathbb R^n\rightarrow \mathbb R^n$ définie par $G(x)=\left(\lvert x_i\rvert^{\frac12}\right)_{1\leq i\leq n}$. La méthode itérative de Newton-Raphson mène à $x_{k+1}=-x_k$. Les détails du calcul sont précisés dans l'annexe \ref{ap:calcul-exemple-nr}.
    \label{ex:newton-raphson-non-differentiable}    
    La méthode Newton-Raphson ne converge pas, et on oscille entre deux points. Effectivement, comme $G$ n'est pas différentiable en $x_k=\overrightarrow 0$, la matrice jacobienne n'est pas valable pour en ce point. Pour toute valeur $x_0\neq \overrightarrow 0$, l'algorithme ne convergera pas. Ci-dessous, les itérations pour la fonction $G$ lorsque $n=1$.
    \begin{figure}[htbp]
        \centering
        \includegraphics[width = 0.3\textwidth]{iteration-newton-2.png}
        \caption{Itérations de la méthode de Newton-Raphson pour $g(x) = \lvert x \rvert ^{\frac12}$ telle que $x_0 = 1$}
        \label{fig:nr-iterations-2}
    \end{figure}
\end{exemple}

\subsubsection{Matrice jacobienne nulle}
La matrice jacobienne peut être nulle, dans ce cas-là la méthode est "bloquée" : $x_{k+1} = x_k$. Ce cas de non-convergence correspond en deux dimensions à la pente nulle de la tangente en $x_k$.

\subsection{Résultats connus}


\subsection{Qualités et faiblesses}
\subsubsection{Convergence}
La méthode de Newton-Raphson est remarquable car sa convergence est quadratique au voisinage de la solution, donc pour un $x_{0}$ suffisament proche de la solution $x^{*}$ la méthode converge très vite: 
\newline
$||x_{k+1}-x^{*} \leq ||x_{k}- x^{*}||^{2}, \qquad \gamma >0$ 
\newline
Cependant, cela n'est vrai que si $x_{0}$ est proche de $x{*}$, ce qui diminue l'interet de cette méthode.
On peut contourner ce problème en ajoutant une phase de recherche linéaire lorsque la Hessienne de $f$: $\nabla ^{2}f$ est définie positive, ce qui n'est pas toujours le cas. Si la Hessienne est définie positive on aura comme direction de descente:
\newline
$d_{k} = -\nabla ^{2}f(x_{k})^{-1} \nabla f(x_{k}) $

\subsubsection{Vers les méthodes de quasi-Newton}
Si la Hessienne de $f$ n'est pas définie positive, il n'est pas possible de faire une recherche linéaire. Cependant il est possible d'en faire une approximation $H_{k]$ ce qui permet de trouver les racines de $f$, les méthodes qui utilisent ces approximations sont des méthodes de quasi-Newton.

\subsubsection{Cas de non convergence}
Dans certains cas, la méthode de Newton-Raphson ne converge pas.
\newline
Si la dérivée première de la fonction $f$: $f'(x)$ n'a pas un comportement normal (par exemple si la fonction n'est pas différenciable au point $x^{*}$) au voisinage de la racine, la méthode ne va pas converger. 
De même, si lors de l'utilisation de la méthode, l'algorithme arrive sur un minimum ou un maximum local, l'algorithme va s'arrêter car la dérivée sera nulle et une division par zéro est impossible.
Toutefois, la non-convergence de la méthode de Newton-Raphson est le plus souvent dûe à une mauvaise estimation de $x_{0}$.

\subsubsection{Cas de convergence lente}
Si la racine recherchée est de multiplicité supérieure à 1, la convergence n'est plus quadratique mais presque linéaire. Cependant, si cette multiplicité est connue il est possible de modifier la méthode pour retrouver une convergence quadratique. En notant $m$ la mutiplicité, on obtient:
\newline
$x_{n+1}=x_{n}-m\frac{f(x_{n})}{f'(x_{n})}$
\newline
Enfin, s'il existe deux racines, proches l'une de l'autre, l'algorithme aura besoin de plusieurs itérations pour s'approcher de l'une d'entre elles.
\subsubsection{Coût de la méthode}
Le coût de la méthode de Newton peut être important. En effet, à chaque itération, il est nécessaire d’évaluer la dérivée de $f$ au point $x_{k}$: $\nabla f(x_{k})$ et la Hessienne de $f$ au point $x_{k}$: $\nabla ^{2}f$. Or, cette évaluation est relativement coûteuse.


\section{Méthode de BFGS}
\subsection{Origines}
\subsection{Principe}
\subsection{Résultats connus}
\subsection{Qualités et faiblesses}
\section{Implémentation sur SCILAB}
\subsection{Recherche linéaire : condition de Wolfe}
\subsubsection{Algorithme}
\subsubsection{Programme}
\subsubsection{Conditions d'arrêts et précision}
\subsubsection{Difficultées techniques}
\subsection{Méthode de Newton}
\subsubsection{Algorithme}
\begin{algorithm}[ht]
    \centering{
        \fbox{
            \begin{minipage}[t]{150mm}
                \footnotesize
                \renewcommand{\baselinestretch}{2.5}
                \resetline
                \begin{tabbing}
                    aaaA\=aaaA\=aaA\=aaA\=aaA\=aaA\kill

                    {\bf  Initialisation :}\\
                    \line{NR-01} \> $g$ := la fonction dont on cherche les racines\\
                    \line{NR-02} \> $\nabla g$ := l'expression analytique du gradient de $g$\\
                    \line{NR-03} \> $k = 0$ := le nombre d'itérations actuels\\
                    \line{NR-04} \> $x_0 \in \mathbb{R}^n$ := la valeur initiale de recherche\\
                    \line{NR-05} \> $\varepsilon \in \mathbb{R}^+_*$ := la marge d'erreur acceptable, le critère d'arrêt\\
                    \line{NR-06} \> $N\in \mathbb{N}$ := le nombre d'itérations maximales\\
                    {\bf Itérations :}\\
                    \line{NR-07} \> {\bf Tant que} $g(x_k) > \varepsilon$ {\bf ET} $k < N$ {\bf faire :}\\
                    \line{NR-08} \>\> $x_{k+1} = x_k - \nabla g^{-1}(x_k)\cdot g(x_k)$\\
                    \line{NR-09} \>\> $N = N+1$\\
                    \line{NR-10} \>\> $k = k+1$\\
                    %------------------
                \end{tabbing}
                \normalsize
            \end{minipage}
        }
        \medbreak
        \caption{Algorithme de la méthode de Newton-Raphson}
        \label{algo:newton_raphson}
    }
\end{algorithm}
\subsubsection{Programme}
\subsubsection{Conditions d'arrêts et précision}
\subsubsection{Difficultées techniques}
\subsection{Méthode de BFGS}
\subsubsection{Algorithme}
\subsubsection{Programme}
\subsubsection{Conditions d'arrêts et précision}
\subsubsection{Difficultées techniques}
\subsection{Affichage des résultats}
\subsubsection{Indicateurs possibles}
\subsection{Exemples d'optimisation}
\subsubsection{Méthode de Newton}
% \paragraph{Exemple convergent}
% \paragraph{Exemple divergent}
% \paragraph{Exemple de convergence difficile}
\subsubsection{Méthode de BFGS}
% \paragraph{Exemple convergent}
% \paragraph{Exemple divergent}
% \paragraph{Exemple de convergence difficile}
\subsection{Critiques et alternatives}
\subsubsection{Critiques}
% \paragraph{Prise en main du langage}
% \paragraph{Temps de calcul}
\subsubsection{Alternatives}
\section{Applications}
\subsection{Problème 1 : ...}
% \paragraph{Indicateur 1 : courbe de convergence (exemple)}

\begin{appendices}
    \section{Détails de calcul de l'exemple \ref{ex:newton-raphson-non-differentiable}}
    \label{ap:calcul-exemple-nr}
    On considère la fonction $G:\mathbb R^n\rightarrow \mathbb R^n$ définie par $G(x)=\left(\lvert x_i\rvert^{\frac12}\right)_{1\leq i\leq n}$.
    \begin{align*}
        \frac{\partial}{\partial x_i}\lvert x_i\rvert^{\frac12} & = \frac{\partial}{\partial x_i}\left(\sqrt{x_i^2}\right)^{\frac12}                                          \\
                                                                & = \frac12\left(\sqrt{x_i^2}\right)^{-\frac12}\cdot\frac{\partial}{\partial x_i}\left(x_i^2\right)^{\frac12} \\
                                                                & = \frac1{2(\lvert x_i\rvert)^{\frac12}}\frac12\cdot2x_i(x_i^2)^{-\frac12}                                   \\
                                                                & = \frac{x_i}{2\lvert x_i\rvert^{\frac32}}                                                                   \\
        \Jacobian_G(x)                                          & = \begin{cases}
            \frac{x_i}{2\lvert x_i\rvert^{\frac32}} & i=j             \\
            0                                       & \textrm{ sinon}
        \end{cases}
    \end{align*}
    $\Jacobian_G$ est une matrice diagonale, son inverse est simplement $\Jacobian^{-1}_G(x)=\begin{cases}\frac{2\lvert x_i\rvert^{\frac32}}{x_i}&i=j\\0&\textrm{ sinon}\end{cases}$

    \begin{align*}
        x_{k+1} & = x_k - \left(\frac{(x_k)_i}{2\lvert (x_k)_i\rvert^{\frac32}}\right)_{1\leq i\leq n}\cdot \lvert (x_k)_i\vert ^{\frac12} \\
                & = x_k - \left(\frac{2\lvert (x_k)_i \rvert^{2}}{(x_k)_i}\right)_{1\leq i\leq n}\\
                & = x_k - 2x_k\\
        x_{k+1} & = - x_k
    \end{align*}
\end{appendices}

% Bibliographie
% http://merkel.texture.rocks/Latex/natbib.php?lang=fr
\bibliographystyle{elsarticle-num-names}
\bibliography{bibliographie}

\phantomsection
\addcontentsline{toc}{section}{Références}

\end{document}
